{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"nf-core/sarek tutorial","text":""},{"location":"#welcome","title":"Welcome","text":"<p>These pages are a tutorial workshop for the Nextflow pipeline nf-core/sarek.</p> <p>In this workshop, we will recap the application of next generation sequencing to identify genetic variations in a genome. You will learn how to use the pipeline sarek to carry out this data-intensive workflow efficiently. We will cover topics such as experimental design, configuration of the pipeline and code execution.</p> <p>Please note that this is not an introductory workshop, and we will assume some basic familiarity with Nextflow.</p> <p>By the end of this workshop, you will have the skills and knowledge to run the sarek workflow and customise some of its features for your own variant calling analyses.</p> <p>Let's get started!</p>"},{"location":"#running-with-gitpod","title":"Running with Gitpod","text":"<p>In order to run this using GitPod, please make sure:</p> <ol> <li>You have a GitHub account: if not, create one here</li> <li>Once you have a GitHub account, sign up for GitPod using your GitHub user here choosing \"continue with GitHub\".</li> </ol> <p>Now you're all set and can use the following button to launch the service:</p> <p></p>"},{"location":"interpretation/","title":"Interpretation","text":"<p>Once variants have been called, the following steps depend on the type of study and the experimental design. For large population studies, like case-control association analyses, an appropriate large-scale statistical approach will be chosen and different statistical or analytical tools will be used to carry out the tertiary analyses.</p> <p>When only a few individuals are involved, and in particular in clinical contexts, the goal will be to interpret the findings in light of different sources of information and pinpoint a causative variant for the investigated phenotype.</p>"},{"location":"interpretation/#overview","title":"Overview","text":"<p>When variants have been called, and a diagnosis is necessary, investigators will need to combine:</p> <ul> <li>the predictions resulting from annotations like the one we carried out</li> <li>biological and clinical information</li> </ul> <p>with the goal of narrowing the search space and reducing the number of variants to be inspected. This approach is summarised in the diagram below:</p> <p>Once the list of variants has been reduced, more in-depth analyses of the reported cases and the genomic region in existing databases might be useful to reach a conclusion.</p>"},{"location":"interpretation/#finding-causative-variants","title":"Finding Causative Variants","text":"<p>Some of these steps might be carried out via software. For this tutorial however, we chose to perform these steps one by one in order to get a better view of the rationale behind this approach.</p> <p>We will start by looking at the annotated VCF, which is found at this location in our GitPod environment:</p> <pre><code>cd /workspace/gitpod/nf-training/variantcalling/annotation/haplotypecaller/joint_variant_calling\n</code></pre> <p>Here, we should verify in which order the two samples we used for this analysis have been written in the VCF file. We can do that by grepping the column names row of the file, and printing at screen the fields from 10th onwards, i.e. the sample columns:</p> <pre><code>zcat joint_germline_recalibrated_snpEff.ann.vcf.gz | grep \"#CHROM\" | cut -f 10-\n</code></pre> <p>This returns:</p> <pre><code>case_case       control_control\n</code></pre> <p>showing that case variants have been written in field 10th and control variants in field 11th.</p> <p>Next, in this educational scenario we might assume that an affected individual (case) will carry at least one alternative allele for the causative variant, while the control individual will be a homozygous for the reference. With this assumption in mind, and a bit of one-liner code, we could first filter the homozygous for the alternative allele in our case, and then the heterozygous.</p> <p>In this first one, we can use the following code:</p> <pre><code>zcat joint_germline_recalibrated_snpEff.ann.vcf.gz | grep PASS | grep HIGH | perl -nae 'if($F[10]=~/0\\/0/ &amp;&amp; $F[9]=~/1\\/1/){print $_;}'\n</code></pre> <p>which results in the following variant.</p> <pre><code>chr21   32576780        rs541034925     A       AC      332.43  PASS    AC=2;AF=0.5;AN=4;DB;DP=94;ExcessHet=0;FS=0;MLEAC=2;MLEAF=0.5;MQ=60;POSITIVE_TRAIN_SITE;QD=33.24;SOR=3.258;VQSLOD=953355.11;culprit=FS;ANN=AC|frameshift_variant|HIGH|TCP10L|ENSG00000242220|transcript|ENST00000300258.8|protein_coding|5/5|c.641dupG|p.Val215fs|745/3805|641/648|214/215||,AC|frameshift_variant|HIGH|CFAP298-TCP10L|ENSG00000265590|transcript|ENST00000673807.1|protein_coding|8/8|c.1163dupG|p.Val389fs|1785/4781|1163/1170|388/389||  GT:AD:DP:GQ:PL  1/1:0,10:10:30:348,30,0 0/0:81,0:81:99:0,119,1600\n</code></pre> <p>Now we can search for this variant in the gnomAD database, which hosts variants and genomic information from sequencing data of almost one million individuals (see v4 release).</p> <p>In order to search for the variant we can type its coordinates in the search field and choose the proposed variant corresponding to the exact position we need. See the figure below:</p> <p></p> <p>the resulting variant data show that our variant is present, and that it's been described already in ClinVar, where the provided interpretation (Clinical Significance) is \"Benign\".</p> <p>We can see the resulting table in the following image:</p> <p></p> <p>Quite importantly, the gnomAD database allows us to gather more information on the gene this variant occurs in. We can inspect the so called \"constraint data\", by clicking on the gene name and inspecting the \"constraint\" table on the top right of the page.</p> <p></p> <p>This information gives us a better view of the selective pressure variation on this gene might be subject to, and therefore inform our understanding of the potential impact of a loss of function variant in this location.</p> <p>In this specific case however the gene is not under purifying selection neither for loss of function variants (LOEUF 0.89) nor for missense ones.</p> <p>We can continue our analysis by looking at the heterozygous variants in our case, for which the control carries a reference homozygous, with the code:</p> <pre><code>zcat joint_germline_recalibrated_snpEff.ann.vcf.gz | grep PASS | grep HIGH | perl -nae 'if($F[10]=~/0\\/0/ &amp;&amp; $F[9]=~/0\\/1/){print $_;}'\n</code></pre> <p>This will results in the following list of variants:</p> <pre><code>chr21   44339194        rs769070783     T       C       57.91   PASS    AC=1;AF=0.25;AN=4;BaseQRankSum=-2.373;DB;DP=84;ExcessHet=0;FS=0;MLEAC=1;MLEAF=0.25;MQ=60;MQRankSum=0;POSITIVE_TRAIN_SITE;QD=3.41;ReadPosRankSum=-0.283;SOR=0.859;VQSLOD=198.85;culprit=FS;ANN=C|start_lost|HIGH|CFAP410|ENSG00000160226|transcript|ENST00000397956.7|protein_coding|1/7|c.1A&gt;G|p.Met1?|200/1634|1/1128|1/375||,C|upstream_gene_variant|MODIFIER|ENSG00000232969|ENSG00000232969|transcript|ENST00000426029.1|pseudogene||n.-182T&gt;C|||||182|,C|downstream_gene_variant|MODIFIER|ENSG00000184441|ENSG00000184441|transcript|ENST00000448927.1|pseudogene||n.*3343T&gt;C|||||3343|;LOF=(CFAP410|ENSG00000160226|1|1.00)   GT:AD:DP:GQ:PL  0/1:8,9:17:66:66,0,71   0/0:67,0:67:99:0,118,999\nchr21   44406660        rs139273180     C       T       35.91   PASS    AC=1;AF=0.25;AN=4;BaseQRankSum=-4.294;DB;DP=127;ExcessHet=0;FS=5.057;MLEAC=1;MLEAF=0.25;MQ=60;MQRankSum=0;POSITIVE_TRAIN_SITE;QD=0.51;ReadPosRankSum=0.526;SOR=1.09;VQSLOD=269.00;culprit=FS;ANN=T|stop_gained|HIGH|TRPM2|ENSG00000142185|transcript|ENST00000397932.6|protein_coding|19/33|c.2857C&gt;T|p.Gln953*|2870/5216|2857/4662|953/1553||;LOF=(TRPM2|ENSG00000142185|1|1.00);NMD=(TRPM2|ENSG00000142185|1|1.00)       GT:AD:DP:GQ:PL  0/1:48,22:71:44:44,0,950        0/0:51,0:51:99:0,100,899\nchr21   45989090        .       C       T       43.91   PASS    AC=1;AF=0.25;AN=4;BaseQRankSum=2.65;DP=89;ExcessHet=0;FS=4.359;MLEAC=1;MLEAF=0.25;MQ=60;MQRankSum=0;QD=2.58;ReadPosRankSum=-1.071;SOR=1.863;VQSLOD=240.19;culprit=FS;ANN=T|stop_gained|HIGH|COL6A1|ENSG00000142156|transcript|ENST00000361866.8|protein_coding|9/35|c.811C&gt;T|p.Arg271*|892/4203|811/3087|271/1028||;LOF=(COL6A1|ENSG00000142156|1|1.00);NMD=(COL6A1|ENSG00000142156|1|1.00)        GT:AD:DP:GQ:PL  0/1:10,7:18:51:52,0,51  0/0:70,0:70:99:0,120,1800\n</code></pre> <p>If we search them one by one, we will see that one in particular occurs on a gene (COL6A1) which was previously reported as constrained for loss of function variants in the database version 2.1:</p> <p></p> <p>while the version 4.0 of the database, resulting from almost one million samples, reports the gene as not constrained:</p> <p></p> <p>We can search for this variant in ClinVar by using an advanced search and limiting our search to both chromosome and base position, like indicated in figure below:</p> <p></p> <p>This will return two results: one deletion and one single nucleotide variant C&gt;T corresponding to the one we called in the case individual:</p> <p></p> <p>If we click on the nomenclature of the variant we found, we will be able to access the data provided with the submission. In this page we can see that multiple submitters have provided an interpretation for this nonsense mutation (2 stars). Under the section \"Submitted interpretations and evidence\" we can gather additional data on the clinical information that led the submitters to classify the variant as \"pathogenic\".</p>"},{"location":"interpretation/#conclusions","title":"Conclusions","text":"<p>After narrowing down our search and inspecting genomic context and clinical information, we can conclude that the variant</p> <pre><code>chr21   45989090        C       T       AC=1;AF=0.25;AN=4;BaseQRankSum=2.37;DP=86;ExcessHet=0;FS=0;MLEAC=1;MLEAF=0.25;MQ=60;MQRankSum=0;QD=2.99;ReadPosRankSum=-0.737;SOR=1.022;VQSLOD=9.09;culprit=QD;ANN=T|stop_gained|HIGH|COL6A1|ENSG00000142156|transcript|ENST00000361866.8|protein_coding|9/35|c.811C&gt;T|p.Arg271*|892/4203|811/3087|271/1028||;LOF=(COL6A1|ENSG00000142156|1|1.00);NMD=(COL6A1|ENSG00000142156|1|1.00)     GT:AD:DP:GQ:PL       0/1:8,6:15:40:50,0,40   0/0:70,0:70:99:0,112,1494\n</code></pre> <p>is most likely the causative one, because it creates a premature stop in the COL6A1 gene, with loss of function variants on this gene known to be pathogenic.</p>"},{"location":"sarek/","title":"Sarek: a Variant Calling Pipeline","text":"<p>In order to carry out a germline variant calling analysis we will use the nf-core pipeline sarek.</p>"},{"location":"sarek/#overview","title":"Overview","text":"<p>The pipeline is organised following the three main analysis blocks we previously described: pre-processing, variant calling and annotation.</p> <p></p> <p>In each analysis block, the user can choose among a range of different options in terms of aligners, callers and software to carry out the annotation. The analysis can also start from different steps, depending the input available and whether it has been partially processed already.</p>"},{"location":"sarek/#experimental-design","title":"Experimental Design","text":"<p>In order to choose the different options Sarek offers, the user should collect a few key elements of the experimental design before beginning the analysis.</p>"},{"location":"sarek/#library-design","title":"Library design","text":"<p>If the experiment used a capture (or targeted) strategy, the user will need to make sure the <code>bed</code> file with the target regions is available. This file will be useful if the user wants to limit variant calling and annotation to those regions. In this case the file can be passed to Sarek command line using the <code>--intervals target.bed</code> parameter. Should the sequencing strategy be a whole exome or panel, the pipeline gives the possibility to enable specific settings for this library type, using the parameter <code>--wes</code>.</p>"},{"location":"sarek/#reference-genome","title":"Reference genome","text":"<p>nf-core pipelines make use of the Illumina iGenomes collection as reference genomes. Before starting the analysis, the user might want to check whether the genome they need is part of this collection. They also might want to consider downloading the reference locally, when running on premises: this would be useful for multiple runs and to speed up the analysis. In this case the parameter <code>--igenomes_base</code> might be used to pass the root directory of the downloaded references.</p> <p>One might also need to use custom files: in this case the user might either provide specific parameters at command line, or create a config file adding a new section to the <code>genome</code> object. See here for more details.</p> <p>We will follow this specific approach in this tutorial, since the data we will be using have been simulated on chromosome 21 of the Human GRCh38 reference, and we have prepared fasta, indexes and annotation files containing only this chromosome locally.</p>"},{"location":"sarek/#input-files","title":"Input files","text":"<p>The input data should be provided in a CSV file, according to a format that is largely common for nf-core pipelines. The format is described in the sarek usage page.</p>"},{"location":"sarek/#gatk-best-practices","title":"GATK Best Practices","text":"<p>During this tutorial we will use the options Sarek offers to follow the GATK best practices workflow.</p> <p>This is solely for educational purposes, since the tutorial dataset includes only 2 samples: while joint-genotyping is a valid choice, the use of soft filtering for such a limited dataset will not offer significant improvements. Additionally, running VQSR on a small dataset will incur in issues with some annotations and will require limiting this step to fewer parameters than usual.</p>"},{"location":"sarek/#running-sarek-germline","title":"Running Sarek Germline","text":"<p>In the following sections we will first prepare our references, then set our computational resources in order to be able to run the pipeline on a gitpod VM, edit the filtering settings and finally run the pipeline.</p>"},{"location":"sarek/#reference-genome_1","title":"Reference Genome","text":"<p>Following the considerations above, we will first of all edit the <code>nextflow.config</code> file in our working directory to add a new genome. It is sufficient to add the following code to the <code>parameters</code> directive in the config.</p> <pre><code>igenomes_base = '/workspace/gitpod/training/data/refs/'\ngenomes {\n    'GRCh38chr21' {\n        bwa                   = \"${params.igenomes_base}/sequence/Homo_sapiens_assembly38_chr21.fasta.{amb,ann,bwt,pac,sa}\"\n        dbsnp                 = \"${params.igenomes_base}/annotations/dbsnp_146.hg38_chr21.vcf.gz\"\n        dbsnp_tbi             = \"${params.igenomes_base}/annotations/dbsnp_146.hg38_chr21.vcf.gz.tbi\"\n        dbsnp_vqsr            = '--resource:dbsnp,known=false,training=true,truth=false,prior=2.0 dbsnp_146.hg38_chr21.vcf.gz'\n        dict                  = \"${params.igenomes_base}/sequence/Homo_sapiens_assembly38_chr21.dict\"\n        fasta                 = \"${params.igenomes_base}/sequence/Homo_sapiens_assembly38_chr21.fasta\"\n        fasta_fai             = \"${params.igenomes_base}/sequence/Homo_sapiens_assembly38_chr21.fasta.fai\"\n        germline_resource     = \"${params.igenomes_base}/annotations/gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only_chr21.vcf.gz\"\n        germline_resource_tbi = \"${params.igenomes_base}/annotations/gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only_chr21.vcf.gz.tbi\"\n        known_snps            = \"${params.igenomes_base}/annotations/1000G_phase1.snps.high_confidence.hg38_chr21.vcf.gz\"\n        known_snps_tbi        = \"${params.igenomes_base}/annotations/1000G_phase1.snps.high_confidence.hg38_chr21.vcf.gz.tbi\"\n        known_snps_vqsr       = '--resource:1000G,known=false,training=true,truth=true,prior=10.0 1000G_phase1.snps.high_confidence.hg38_chr21.vcf.gz'\n        known_indels          = \"${params.igenomes_base}/annotations/Mills_and_1000G_gold_standard.indels.hg38_chr21.vcf.gz\"\n        known_indels_tbi      = \"${params.igenomes_base}/annotations/Mills_and_1000G_gold_standard.indels.hg38_chr21.vcf.gz.tbi\"\n        known_indels_vqsr     = '--resource:mills,known=false,training=true,truth=true,prior=10.0 Mills_and_1000G_gold_standard.indels.hg38_chr21.vcf.gz'\n        snpeff_db             = '105'\n        snpeff_genome         = 'GRCh38'\n    }\n}\n</code></pre>"},{"location":"sarek/#computing-resources","title":"Computing resources","text":"<p>Based on the choices we made when starting up the gitpod environment, we recommend to use the following additional parameters. They can also be added to the parameters directive in the config file we just edited.</p> <pre><code>params {\n    max_cpus                  = 2\n    max_memory                = '6.5GB'\n    max_time                  = '2.h'\n    use_annotation_cache_keys = true\n}\n</code></pre> <p>The parameter <code>use_annotation_cache_keys</code> allows the annotation software to deal with the local paths when the cache is downloaded on the environment.</p>"},{"location":"sarek/#filtering-parameters","title":"Filtering parameters","text":"<p>As we mentioned earlier, we will be using the VQSR filtering tool once the variants have been called. However, this tool should be used to take advantage of larger amount of variant annotations and improve filtering: when a small tutorial dataset is used, some of the annotations will not have sufficient data or might even have no variance. In order to account for this, we have to change the filtering options and limit this approach to a subset of variant annotations.</p> <p>We can do this by editing the process descriptors for the Sarek modules running VQSR for both single nucleotide variants and insertion/deletions.</p> <pre><code>process {\n    withName: 'VARIANTRECALIBRATOR_INDEL' {\n        ext.prefix = { \"${meta.id}_INDEL\" }\n        ext.args   = \"-an QD -an FS -an SOR -an DP  -mode INDEL\"\n        publishDir = [\n            enabled: false\n        ]\n    }\n\n    withName: 'VARIANTRECALIBRATOR_SNP' {\n        ext.prefix = { \"${meta.id}_SNP\" }\n        ext.args   = \"-an QD -an MQ -an FS -an SOR -mode SNP\"\n        publishDir = [\n            enabled: false\n        ]\n    }\n}\n</code></pre>"},{"location":"sarek/#launching-the-pipeline","title":"Launching the pipeline","text":"<p>Now we are ready to launch the pipeline, and we can use the following command line:</p> <pre><code>nextflow run nf-core/sarek \\\n--input /workspace/gitpod/training/data/reads/sarek-input.csv \\\n--outdir . \\\n--tools haplotypecaller,snpeff \\\n--genome GRCh38chr21 \\\n--joint_germline \\\n--intervals /workspace/gitpod/training/exome_target_hg38_chr21.bed \\\n--wes\n</code></pre> <p>Notice that we have selected <code>--joint_germline</code> to enable the joint-genotyping workflow, we have specified our library strategy is using a capture with <code>--wes</code> and we have provided a bed file with the targets with <code>--intervals</code>. The target file in this case refers to the capture intervals on chromosome 21 only, where the data have been simulated.</p> <p>The whole pipeline from FASTQ input to annotated VCF should run in about 25 minutes.</p> <p>Our final VCF file will be located in</p> <pre><code>./annotation/haplotypecaller/joint_variant_calling\n</code></pre>"},{"location":"theory/","title":"Calling Variants on Sequencing Data","text":"<p>Before we dive into one of the nf-core pipelines used for variant calling, it's worth looking at some theoretical aspects of variant calling.</p>"},{"location":"theory/#overview","title":"Overview","text":"<p>The term \"variant calling\" is rooted in the history of DNA sequencing, and it indicates an approach where we identify (i.e. call) positions in a genome (loci) which are variable in a population (genetic variants). The specific genotype of an individual at that variant locus is then assigned.</p> <p>There are many different approaches for calling variants from sequencing data: here, we will look more specifically at a reference-based variant calling approach, i.e. where a reference genome is needed and variant sites are identified by comparing the reads to this reference.</p> <p>Over the years, also thanks to the work carried out by the GATK team at the Broad Institute, there has been a convergence on a \"best practices\" workflow, which is summarised in the diagram below:</p> <p></p> <p>In this scheme we can identify a few key phases in the workflow. Pre-processing is the first part, where raw data are handled and mapped to a genome reference, to be then transformed in order to increase the accuracy of the following analyses. Then, variant calling is carried out. This is followed by filtering and annotation. Here we will briefly discuss these key steps, which might vary depending on the specific type of data one is performing variant calling on.</p>"},{"location":"theory/#alignment","title":"Alignment","text":"<p>The alignment step is where reads obtained from genome fragments of a sample are identified as originating from a specific location in the genome. This step is essential in a reference-based workflow, because it is the comparison of the raw data with the reference to inform us on whether a position in the genome might be variable or not.</p> <p>Mismatches, insertions and deletions (INDELs) as well as duplicated regions make this step sometimes challenging: this is the reason why an appropriate aligner has to be chosen, depending on the sequencing application and data type.</p> <p>Once each raw read has been aligned to the region of the genome it is most likely originating from, the sequence of all reads overlapping each locus can be used to identify potentially variable sites. Each read will support the presence of an allele identical to the reference, or a different one (alternative allele), and the variant calling algorithm will measure the weighted support for each allele.</p> <p>However, the support given by the raw data to alternative variants might be biased. For this reason, one can apply certain corrections to the data to ensure the support for the alleles is assessed correctly. This is done by performing the two steps described below: marking duplicates, and recalibrating base quality scores.</p>"},{"location":"theory/#marking-duplicates","title":"Marking Duplicates","text":"<p>Duplicates are non-independent measurements of a sequence fragment.</p> <p>Since DNA fragmentation is theoretically random, reads originating from different fragments provide independent information. An algorithm can use this information to assess the support for different alleles. When these measurements however are not independent, the raw data might provide a biased support towards a specific allele.</p> <p>Duplicates can be caused by PCR during library preparation (library duplicates) or might occur during sequencing, when the instrument is reading the signal from different clusters (as in Illumina short read sequencing). These latter are called \"optical duplicates\".</p> <p>A specific step called \"marking duplicates\" identifies these identical pairs using their orientation and 5' position (before any clipping), which will be assumed to be coming from the same input DNA template: one representative pair is then chosen based on quality scores and other criteria, while the other ones are marked. Marked reads are then ignored in the following steps.</p>"},{"location":"theory/#base-quality-score-recalibration","title":"Base Quality Score Recalibration","text":"<p>Among the parameters used by a variant calling algorithm to weigh the support for different alleles, the quality score of the base in the read at the variant locus is quite important. Sequencing instruments, however, can make systematic errors when reading the signal at each cycle, and cannot account for errors originated in PCR.</p> <p>Once a read has been aligned to the reference, an appropriate algorithm can however compare the error rate estimated from the existing base quality scores, with the actual differences observed with the reference sequence (empirical quality), and perform appropriate corrections. This process is called \"base quality score recalibration\" (BQSR).</p> <p>To calculate empirical qualities, the algorithm simply counts the number of mismatches in the observed bases. Any mismatch which does not overlap a known variant is considered an error. The empirical error rate is simply the ratio between counted errors and the total observed bases. A Yates correction is applied to this, to avoid either dividing by 0 or dealing with small counts.</p> \\[ e_{empirical} = \\frac{n_{mismatches} + 1}{n_{bases} +2} \\] <p>The empirical error is expressed as a Quality in Phred-scale:</p> \\[ Q_{empirical} = -10 \\times log_{10}(e_{empirical}) \\] <p>Let's use a simple example like the one in the diagram below, where for illustrative purposes we only consider the bases belonging to the same read.</p> <p></p> <p>In this example we have 3 mismatches, but one is a reported variant site: we therefore only count 2 errors, over 10 observed bases. According to the approach we just explained,</p> \\[ Q_{empirical} = -10 \\times log_{10}(\\frac{2 + 1}{10 +2}) = 6.29 \\] <p>To calculate the average reported Q score, we should sum the error probabilities and then convert them back into phred scale:</p> \\[ Q_{average} = -10 \\times log_{10}(\\frac {0.1 + 0.1 + 0.01 + 0.1 + 0.01 + 0.01 + 0.01 + 0.1 + 0.1 + 0.1}{10}) = 11.94 \\] <p>Our empirical Q score would be 6.29, the average reported Q score is 11.94, and therefore the \\(\\Delta = 11.94 - 6.29 = 5.65\\)</p> <p>The recalibrated Q score of each base would correspond to the reported Q score minus this \\(\\Delta\\).</p> <p>In a real sequencing dataset, this calculation is performed for different groups (bins) of bases: those in the same lane, those with the same original quality score, per machine cycle, per sequencing context. In each bin, the difference (\\(\\Delta\\)) between the average reported quality and the empirical quality is calculated. The recalibrated score would then be the reported score minus the sum of all deltas calculated in each bin the base belongs to.</p> <p>A detailed summary of this approach can be found on the GATK BQSR page. We also found quite useful this step by step guide through the matematical approach. Full details are explained in the publication that first proposed this method.</p>"},{"location":"theory/#calling-variants","title":"Calling Variants","text":"<p>Once we have prepared the data for an accurate identification of the variants, we are ready to perform the next steps. The most important innovation introduced some years ago in this part of the workflow, has been to separate the identification of a variant site (i.e. variant calling itself) from the assignment of the genotype to each individual. This approach makes the computation more approachable, especially for large sample cohorts: BAM files are only accessed per-sample in the first step, while multi-sample cohort data are used together in the second step in order to increase the accuracy of genotype assignment.</p>"},{"location":"theory/#identifying-variants","title":"Identifying Variants","text":"<p>In this phase, which is performed on each sample independently, a first step uses a sliding window to count differences compared to the reference (i.e. mismatches, INDELs) and potentially variable regions are identified. GATK calls these \"active regions\". Then, a local graph assembly of the reads is created to identify plausible haplotypes, which are aligned to the reference with a traditional alignment algorithm called \"Smith-Waterman\": this is used to identify variants. For each read in an active region, the support for each of the haplotypes is counted and a likelihood score for each combination of read/haplotype is calculated. The likelihoods at this step allow to calculate the support for each of the alleles in a variant site, and read-haplotype likelihoods are a key input for the Bayesian statistics used to determine the most likely genotype. This first genotype assignment could be sufficient if one analysed a single sample only.</p>"},{"location":"theory/#assigning-genotypes","title":"Assigning Genotypes","text":"<p>When multiple samples are analysed, information from each of them could collectively improve the genotype assignment. This is because the magnitude of potential biases (example: strand bias) can be better estimated, and because the distributions of those annotations used to inform the genotype assignment become more stable when more data are available, by combining multiple samples. The use of a larger cohort also increases the sensitivity.</p> <p>This is possible if the variant calling step is run by producing a variation of the VCF file format called GVCF: this format includes, in addition to variant sites, also non-variant intervals in the genome of each sample. Moreover, it reports probability likelihoods of a non-reference symbolic allele at these non-variant intervals. This information allows to re-genotype each sample by using data from the whole cohort.</p> <p>You can read more on the GATK website about the logic of joint calling.</p>"},{"location":"theory/#filtering-variants","title":"Filtering Variants","text":"<p>There are several ways to spot potential false positives through filtering.</p> <p>Hard filtering uses pre-defined thresholds of different variant annotations (allele-depth, mapping quality and many others) in order to flag variants passing all these criteria, and those failing to meet any of them. This approach is mostly useful when calling a few samples and enough data are not available for more sophisticated solutions.</p> <p>Soft filtering infers the thresholds to be applied from the data themselves. This approach uses the distributions of the annotations, and their overlap with known and validated variants: it defines those combinations of annotations which are more likely to describe true positives (the variants they refer to in the analysis cohort overlap with those validated in other databases). This approach is used by a GATK tool called Variant Quality Score Recalibration (VQSR).</p> <p>More details can be found on the GATK VQSR page.</p> <p>More recently, pre-trained deep learning models are also available to filter variants based on neural network architectures trained on a large number of variants from population databases.</p>"},{"location":"theory/#annotation","title":"Annotation","text":"<p>Once the analysis has produced a final VCF file, the final step which is necessary to interpret the results is called \"annotation\". This step uses different databases to describe (annotate) each variant from a genomic, biological, or population point of view. The software used to carry out this task will add information to the VCF file such as:</p> <ul> <li>the gene each variant overlaps with</li> <li>the transcript the variant overlaps with</li> <li>the potential biological consequence on each of those transcripts</li> <li>population frequency (minor allele frequency, described in different databases such as gnomAD)</li> </ul> <p>And several other items we can use to interpret our findings from a biological or clinical point of view.</p>"}]}